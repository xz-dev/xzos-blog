---
title: "AI 时代的手动测试革命"
pubDate: "2025-07-18"
description: "当 GitHub Copilot 正在为我自动进行手动测试时，我为自己发明了一个 AI Agent for 手动测试。探索如何利用 AI 工具提升测试效率。"
author: "xz-dev"
category: "AI"
tags: ["AI", "AI Agent", "MCP", "Prompt"]
---

> 当我在写这篇文档的时候，GitHub copilot 正在为我自动进行手动测试
>
> 可以说，我为自己发明了一个 AI Agent for 手动测试。

## 前言

自从 GPT 出现，很多人说，程序员将会失业。但很少有人注意到，软件测试，才是这个时代第一个受到冲击的互联网职业。

我预言，就如发电厂的工人，软件测试工程师将从需要亲力亲为，变成 AI 测试的监督者，但人是不会在这里消失的。

<!--more-->

## 背景

大家都知道，我是一名程序员。

本来我正在为微软做一个基于 AI 的自动生成自动化测试代码的工具。但是在某天后领导突然认为我们无法让程序在 20 天的期限内完成了。于是我有了一个临时而"紧急"的测试任务：需要 20 天内手动测试 400 个 Microsoft Graph API 的接口。

这并不难，但是需要仔细研究每个接口如何调用，作用是什么，以及如何设置前置环境进行测试，这并不简单。

而且，我讨厌手动。手动意味着聚精会神地浪费时间去学习以后也不会用到的技巧和知识。

## 解决方案

### 前提

- 我必须手动测试，400 个端口带来的自动测试的代码量是惊人的，包含各种错误的冗余和自动生成文档的代码
- 我必须不能手动去测试
- 当 AI 错了，我需要及时纠正它

那么，这看起来像什么，是的，AI 辅助编程工具。

### 思路

- AI 用哪个：GitHub Copilot
- 如何编写文档：VSCode，pandoc（markdown to docx）
- 如何让 AI 理解需求：单独的 prompt.md 提示词文档和 example.md 格式文档
- 如何让 AI 操作测试工具：MCP 工具（playwright，这还不够好）

### 实现

#### MCP 工具

##### [Auto Azure Export](https://github.com/MS-Xiangzhe/auto_azure_export)

playwright-mcp 足以，但是考虑到大量的文档和测试用例，针对特定的需求，它还不够好。

作为一个很烂的提示词工程师，我需要让 AI 只能接触到少量的数据，这样它就不会错出个花来。

所以，花1小时，用 GitHub copilot 编写一个基于 playwright 的 MCP 套壳，只保留了最小的函数：

- 设置请求 URL
- 设置请求方法（GET/POST/…）
- 设置请求体
- 点击请求按钮
- 获取返回的 HTTP 状态码
- 获取返回的 HTTP body

##### [Playwright MCP](https://github.com/microsoft/playwright-mcp)

方便 AI 遇到问题了自己查阅文档。

### 优化

#### prompt.md

用口语一句话一行表达自己的想法，比如：

```
你是一个专业的测试，需要测试 Microsoft Graph API 的接口文档。
你需要使用 MCP 工具测试。
开始做之前先告诉我你将要怎么开始做。
每次只完成一个 API 端点的测试和文档，然后询问我是否继续。
你需要用 mem.json 记住/更新你认为重要的知识。
...
```

然后，让 AI 来自己优化这段 prompt。它会自动检查所有文件，然后，用自己能看懂的方式重新复述，比如测试步骤应该是 1. 2. ….

prompt.md 最关键的作用是建立了严格的标准化流程。无论重新开始多少次对话，AI 都能保持一致的工作方式，这是大规模应用的基础。

##### 重点提示词

- 角色（现在的 AI 都是混合型专家，确保你的 AI 总是调用专业的那个专家模型，以及节约在 token 中搜索的时间）
- 开始做之前先复述（确保 AI 不会失控，而你的 token 不会白花）
- 每次只完成一个小任务（确保 AI 不会突然胡言/崩溃/网络错误…，然后白花你的时间）

#### mem.json

让 AI 像人类测试一样逐渐进步，变得越来越准确。

通过 mem.json 机制，AI 不再是"无记忆"的工具，而是能够积累经验、避免重复错误的"学习型助手"。

这让 AI 测试的效率随时间递增。

#### 完全体

![AI Agent for Test: VSCode + GitHub Copilot + MCP](/images/blog/ai-driven-manual-testing-revolution/diagram.png)

## 上手试试

[下载示例项目 azure_graph_api.zip](https://web.archive.org/web/20250813221116/https://xzos.net/wp-content/uploads/2025/07/azure_graph_api.zip)

解压它，然后查看 ai_prompt/ask_ai.txt ， 使用 sonnet 4，感受"魔法"。

## 总结

- 请首先确定 AI 模型，并坚持使用。对于提示词而言，AI 模型很重要。相同的提示词，GPT o4 可能就会开始胡言乱语。
- 提示词就是"记忆"。对于 AI 而言，提示词很重要。AI 不是人类，他们无法真的通过对话"记住"什么。所以我们需要一个文件来帮助他。
- 适时放弃你的 AI。当你发现 AI 突然变傻，不要尝试"拯救"你的上下文。放弃它，然后带着你的文档和 mem.json 重新开始。
- AI 并不擅长遵守规定。很多时候 AI 会开始自由发挥，这就是为什么我们需要第二点和第三点，以及需要一个不那么"智能"的 MCP。

### 还能再改进吗

- 更加自动化。我们可以再部署一个本地 AI Agent 并代理整个 VSCode 的 GUI 界面和聊天框，来给我们自动输入"继续"、点击 Continue和监视 copilot。当什么东西出错时，本地 AI Agent 再提醒我们，这样我们就实现了手动测试的"监督者"时代。
- 用定制化的 MCP 去编辑文档，保证文档格式。如果这是需要长期运行的工具，请实现一个生成文档的 MCP，因为你不会希望你的 AI 随便乱改你的文档格式，而你也不想时时刻刻盯着或者检查。

### 那未来呢

我相信，未来的 AI 能意识到。自己编写 MCP 来规范化自己的行为。

## 感言

也许某一天，算力使得程序可以不不要那么优化，那么程序将由 AI 生成。而 GitHub 或者类似的开源平台，也不再热衷分享代码逻辑，而是提示词与 AI 模型。
